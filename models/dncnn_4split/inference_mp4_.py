import os
import time
import cv2
import numpy as np
from tqdm import tqdm
from datetime import datetime
from hailo_platform import (
    HEF,
    ConfigureParams,
    Device,
    FormatType,
    HailoSchedulingAlgorithm,
    HailoStreamInterface,
    InferVStreams,
    InputVStreamParams,
    OutputVStreamParams,
    VDevice,
)

# -------------------------- æ ¸å¿ƒé…ç½®å‚æ•° --------------------------
# è¾“å…¥è§†é¢‘å‚æ•°
INPUT_VIDEO_PATH = "/home/firefly/Denoising-rk3588J/data/20250703video/WIN_20250703_17_58_53_Pro.mp4"
TARGET_RESOLUTION = (960, 720)       # éœ€ä¸æ¨¡å‹è¾“å…¥åŒ¹é…

# æ¨ç†å‚æ•°ï¼ˆå•è®¾å¤‡ä¸²è¡Œï¼Œæ— éœ€å¤šè®¾å¤‡é…ç½®ï¼‰
HEF_PATH = "/home/firefly/Denoising-rk3588J/models/dncnn_4split/dncnn_4split_16pad.hef"
DEVICE_ID = 1                        # ä½¿ç”¨å•ä¸ªHailoè®¾å¤‡ï¼ˆæ ¹æ®å®é™…è®¾å¤‡IDè°ƒæ•´ï¼‰

# è¾“å‡ºè§†é¢‘å‚æ•°
SAVE_VIDEO_DIR = "output/inference_videos"
VIDEO_CODEC = cv2.VideoWriter_fourcc(*"mp4v")
VIDEO_EXT = ".mp4"
SPLIT_LINE_COLOR = (0, 255, 0)        # ç»¿è‰²åˆ†å‰²çº¿
SPLIT_LINE_WIDTH = 2
TEXT_COLOR = (255, 255, 255)          # ç™½è‰²æ–‡å­—
TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX
TEXT_SIZE = 0.8
TEXT_THICKNESS = 2


# -------------------------- å¸§é¢„å¤„ç† --------------------------
def process_frame(frame):
    """å¤„ç†è§†é¢‘BGRå¸§ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆRGB+float32ï¼‰"""
    # è°ƒæ•´åˆ†è¾¨ç‡
    frame_resized = cv2.resize(
        frame, 
        dsize=TARGET_RESOLUTION, 
        interpolation=cv2.INTER_LANCZOS4
    )
    
    # BGRè½¬RGB
    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

    # æ ¼å¼è½¬æ¢ä¸ºfloat32
    frame_float = frame_rgb.astype(np.float32)
    
    return frame_float, frame_resized  # è¿”å›é¢„å¤„ç†å¸§ï¼ˆæ¨ç†ç”¨ï¼‰å’ŒåŸå§‹è°ƒæ•´å¸§ï¼ˆæ‹¼æ¥ç”¨ï¼‰


# -------------------------- æ¨ç†ç»“æœåå¤„ç† --------------------------
def postprocess_infer_result(infer_tensor):
    """å°†æ¨ç†è¾“å‡ºå¼ é‡è½¬æ¢ä¸ºBGRå¸§ï¼ˆé€‚é…OpenCVï¼‰"""
    # ç§»é™¤å¤šä½™ç»´åº¦
    tensor_squeezed = np.squeeze(infer_tensor)
    
    # CHW â†’ HWCè½¬æ¢
    if tensor_squeezed.shape[0] in [3, 1]:
        frame_hwc = tensor_squeezed.transpose(1, 2, 0)
    else:
        frame_hwc = tensor_squeezed
    
    # å•é€šé“è½¬ä¸‰é€šé“
    if frame_hwc.shape[-1] == 1:
        frame_hwc = np.repeat(frame_hwc, 3, axis=-1)
    
    # åˆ†è¾¨ç‡æ ¡å‡†
    frame_resized = cv2.resize(
        frame_hwc, 
        dsize=TARGET_RESOLUTION, 
        interpolation=cv2.INTER_LANCZOS4
    )
    
    # RGBâ†’BGR
    frame_bgr = cv2.cvtColor(frame_resized.astype(np.uint8), cv2.COLOR_RGB2BGR)
    return frame_bgr


# -------------------------- å¸§åˆ†å‰²ä¸æ‹¼æ¥ï¼ˆé€‚é…4åˆ†å—æ¨¡å‹ï¼‰ --------------------------
def split_and_stack(frame_tensor):
    """å°†[720, 960, 3]çš„å•å¸§åˆ†å‰²ä¸º4ä¸ª[376, 496, 3]å­å›¾ï¼Œå †å ä¸º[4, 376, 496, 3]"""
    if frame_tensor.shape != (720, 960, 3):
        raise ValueError(f"è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º[720, 960, 3]ï¼Œå®é™…ä¸º{frame_tensor.shape}")
    
    # åˆ†å—å‚æ•°ï¼ˆå¸¦16åƒç´ paddingï¼‰
    sub_height = 720 // 2 + 16  # 376
    sub_width = 960 // 2 + 16   # 496
    
    # åˆ†å‰²4ä¸ªå­å›¾
    sub1 = frame_tensor[:sub_height, :sub_width, :]          # å·¦ä¸Š
    sub2 = frame_tensor[-sub_height:, :sub_width, :]         # å·¦ä¸‹
    sub3 = frame_tensor[:sub_height, -sub_width:, :]         # å³ä¸Š
    sub4 = frame_tensor[-sub_height:, -sub_width:, :]        # å³ä¸‹
    
    # å †å ä¸ºæ‰¹æ¬¡æ ¼å¼ï¼ˆé€‚é…æ¨¡å‹è¾“å…¥ï¼‰
    return np.stack([sub1, sub2, sub3, sub4], axis=0)


def stack_to_original(sub_images):
    """å°†[4, 376, 496, 3]çš„å­å›¾æ‹¼æ¥å›åŸå§‹[720, 960, 3]å¸§"""
    if sub_images.shape != (4, 376, 496, 3):
        raise ValueError(f"è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º[4, 376, 496, 3]ï¼Œå®é™…ä¸º{sub_images.shape}")
    
    sub1, sub2, sub3, sub4 = sub_images[0], sub_images[1], sub_images[2], sub_images[3]
    
    # å»é™¤padding
    sub1_crop = sub1[:360, :480, :]
    sub2_crop = sub2[16:, :480, :]
    sub3_crop = sub3[:360, 16:, :]
    sub4_crop = sub4[16:, 16:, :]
    
    # æ‹¼æ¥
    top_row = np.concatenate([sub1_crop, sub3_crop], axis=1)
    bottom_row = np.concatenate([sub2_crop, sub4_crop], axis=1)
    full_image = np.concatenate([top_row, bottom_row], axis=0)
    
    return full_image


# -------------------------- å¸§æ‹¼æ¥ä¸æ ‡æ³¨ --------------------------
def stitch_frames(original_frame, infer_frame, process_fps):
    """æ¨ªå‘æ‹¼æ¥åŸå§‹å¸§ä¸æ¨ç†å¸§ï¼Œå¹¶æ·»åŠ æ ‡æ³¨"""
    # æ¨ªå‘æ‹¼æ¥
    stitched_frame = cv2.hconcat([original_frame, infer_frame])
    
    # æ·»åŠ åˆ†å‰²çº¿
    split_x = original_frame.shape[1]
    cv2.line(
        stitched_frame, 
        (split_x, 0), 
        (split_x, stitched_frame.shape[0]), 
        SPLIT_LINE_COLOR, 
        SPLIT_LINE_WIDTH
    )
    
    # æ·»åŠ æ–‡å­—æ ‡æ³¨
    cv2.putText(stitched_frame, "Original Frame", (20, 40),
                TEXT_FONT, TEXT_SIZE, TEXT_COLOR, TEXT_THICKNESS)
    cv2.putText(stitched_frame, "Inferred Frame", (split_x + 20, 40),
                TEXT_FONT, TEXT_SIZE, TEXT_COLOR, TEXT_THICKNESS)
    
    return stitched_frame


# -------------------------- Hailoè®¾å¤‡åˆå§‹åŒ–ï¼ˆå•è®¾å¤‡ï¼‰ --------------------------
def init_single_device(hef_path, device_id):
    """åˆå§‹åŒ–å•ä¸ªHailoè®¾å¤‡å¹¶åŠ è½½æ¨¡å‹"""
    # æ‰«æå¯ç”¨è®¾å¤‡
    device_ids = Device.scan()
    if len(device_ids) <= device_id:
        raise RuntimeError(f"è®¾å¤‡ID {device_id} ä¸å­˜åœ¨ï¼Œä»…æ£€æµ‹åˆ° {len(device_ids)} ä¸ªHailoè®¾å¤‡")
    
    print(f"åˆå§‹åŒ–è®¾å¤‡ {device_id}ï¼ˆç¡¬ä»¶ID: {device_ids[device_id]}ï¼‰...")
    
    # åˆ›å»ºè®¾å¤‡å‚æ•°
    vdevice_params = VDevice.create_params()
    vdevice_params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE
    vdevice_params.device_ids.append(device_id)
    target = VDevice(params=vdevice_params)
    
    # åŠ è½½HEFæ¨¡å‹
    hef = HEF(hef_path)
    
    # é…ç½®ç½‘ç»œç»„
    configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)
    network_groups = target.configure(hef, configure_params)
    network_group = network_groups[0]
    network_group_params = network_group.create_params()
    
    # åˆ›å»ºè¾“å…¥/è¾“å‡ºæµå‚æ•°
    input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)
    output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)
    
    # è·å–æµä¿¡æ¯
    input_vstream_info = hef.get_input_vstream_infos()[0]
    output_vstream_info = hef.get_output_vstream_infos()[0]
    
    device_info = {
        "target": target,
        "network_group": network_group,
        "network_group_params": network_group_params,
        "input_vstreams_params": input_vstreams_params,
        "output_vstreams_params": output_vstreams_params,
        "input_vstream_info": input_vstream_info,
        "output_vstream_info": output_vstream_info
    }
    
    print(f"è®¾å¤‡åˆå§‹åŒ–å®Œæˆ | è¾“å…¥å½¢çŠ¶: {input_vstream_info.shape} | è¾“å‡ºå½¢çŠ¶: {output_vstream_info.shape}")
    return device_info


# -------------------------- å•å¸§æ¨ç†å‡½æ•° --------------------------
def run_single_frame_inference(device, frame_tensor):
    """å¯¹å•å¸§è¿›è¡Œæ¨ç†ï¼ˆå«åˆ†å—ä¸æ‹¼æ¥ï¼‰"""
    # 1. å¸§åˆ†å—ï¼ˆé€‚é…4åˆ†å—æ¨¡å‹ï¼‰
    split_tensor = split_and_stack(frame_tensor)
    
    # 2. æ‰§è¡Œæ¨ç†
    network_group = device["network_group"]
    input_vstreams_params = device["input_vstreams_params"]
    output_vstreams_params = device["output_vstreams_params"]
    network_group_params = device["network_group_params"]
    input_vstream_info = device["input_vstream_info"]
    
    start_time = time.time()
    with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:
        with network_group.activate(network_group_params):
            input_data = {input_vstream_info.name: split_tensor}
            infer_results = infer_pipeline.infer(input_data)
    infer_time = time.time() - start_time
    
    # 3. æ¨ç†ç»“æœæ‹¼æ¥å›åŸå§‹å°ºå¯¸
    infer_tensor = infer_results[device["output_vstream_info"].name]
    original_infer_frame = stack_to_original(infer_tensor)
    
    return original_infer_frame, infer_time


# -------------------------- ä¸»å‡½æ•°ï¼ˆä¸²è¡Œå¤„ç†é€»è¾‘ï¼‰ --------------------------
def main():
    total_start_time = time.time()
    print("="*60)
    print(f"å¼€å§‹ä¸²è¡Œè§†é¢‘å¤„ç† | ç›®æ ‡åˆ†è¾¨ç‡: {TARGET_RESOLUTION[0]}Ã—{TARGET_RESOLUTION[1]}")
    print(f"ä½¿ç”¨è®¾å¤‡ID: {DEVICE_ID} | æ¨¡å‹è·¯å¾„: {HEF_PATH}")
    print(f"è¾“å…¥è§†é¢‘: {INPUT_VIDEO_PATH} | è¾“å‡ºç›®å½•: {SAVE_VIDEO_DIR}")
    print("="*60)

    # 1. æ£€æŸ¥è¾“å…¥è§†é¢‘
    if not os.path.exists(INPUT_VIDEO_PATH):
        raise FileNotFoundError(f"è¾“å…¥è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_VIDEO_PATH}")

    # 2. åˆå§‹åŒ–è¾“å‡ºç›®å½•
    os.makedirs(SAVE_VIDEO_DIR, exist_ok=True)
    
    # 3. æ‰“å¼€è¾“å…¥è§†é¢‘
    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)
    if not cap.isOpened():
        raise RuntimeError(f"æ— æ³•æ‰“å¼€è¾“å…¥è§†é¢‘: {INPUT_VIDEO_PATH}")
    
    # 4. è·å–è¾“å…¥è§†é¢‘ä¿¡æ¯
    input_fps = cap.get(cv2.CAP_PROP_FPS)
    input_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    input_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"\nè¾“å…¥è§†é¢‘ä¿¡æ¯ | åˆ†è¾¨ç‡: {input_width}Ã—{input_height} | FPS: {input_fps:.1f} | æ€»å¸§æ•°: {total_frames}")

    # 5. åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
    video_filename = f"serial_infer_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}{VIDEO_EXT}"
    video_save_path = os.path.join(SAVE_VIDEO_DIR, video_filename)
    stitched_resolution = (TARGET_RESOLUTION[0] * 2, TARGET_RESOLUTION[1])  # æ¨ªå‘æ‹¼æ¥
    
    video_writer = cv2.VideoWriter(
        video_save_path,
        VIDEO_CODEC,
        input_fps,
        stitched_resolution
    )
    
    if not video_writer.isOpened():
        raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨: {video_save_path}")
    print(f"è¾“å‡ºè§†é¢‘ä¿¡æ¯ | åˆ†è¾¨ç‡: {stitched_resolution[0]}Ã—{stitched_resolution[1]} | ä¿å­˜è·¯å¾„: {video_save_path}")

    # 6. åˆå§‹åŒ–Hailoè®¾å¤‡
    device = init_single_device(HEF_PATH, DEVICE_ID)

    # 7. åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    processed_frames = 0
    total_infer_time = 0.0
    start_process_time = time.time()

    # 8. ä¸²è¡Œå¤„ç†ï¼šè¯»å–â†’æ¨ç†â†’å†™å…¥
    print(f"\nå¼€å§‹ä¸²è¡Œå¤„ç†ï¼ˆå…±{total_frames}å¸§ï¼‰...")
    progress = tqdm(total=total_frames, desc="å¤„ç†è¿›åº¦")
    
    while True:
        # æ­¥éª¤1ï¼šè¯»å–ä¸€å¸§
        ret, frame = cap.read()
        if not ret:
            break  # è§†é¢‘è¯»å–å®Œæ¯•
        
        # æ­¥éª¤2ï¼šé¢„å¤„ç†å¸§
        frame_processed, frame_original = process_frame(frame)
        
        # æ­¥éª¤3ï¼šå•å¸§æ¨ç†ï¼ˆå«åˆ†å—ä¸æ‹¼æ¥ï¼‰
        infer_frame_tensor, infer_time = run_single_frame_inference(device, frame_processed)
        total_infer_time += infer_time
        
        # æ­¥éª¤4ï¼šåå¤„ç†æ¨ç†ç»“æœ
        infer_frame = postprocess_infer_result(infer_frame_tensor)
        
        # æ­¥éª¤5ï¼šè®¡ç®—å½“å‰å¤„ç†FPS
        elapsed_time = time.time() - start_process_time
        current_fps = processed_frames / elapsed_time if elapsed_time > 1e-3 else 0.0
        
        # æ­¥éª¤6ï¼šæ‹¼æ¥å¸§å¹¶å†™å…¥è§†é¢‘
        stitched_frame = stitch_frames(frame_original, infer_frame, current_fps)
        video_writer.write(stitched_frame)
        
        # æ­¥éª¤7ï¼šæ›´æ–°ç»Ÿè®¡ä¸è¿›åº¦
        processed_frames += 1
        progress.update(1)

    # 9. é‡Šæ”¾èµ„æº
    progress.close()
    video_writer.release()
    cap.release()
    device["target"].release()  # é‡Šæ”¾Hailoè®¾å¤‡
    cv2.destroyAllWindows()

    # 10. è¾“å‡ºç»Ÿè®¡ç»“æœ
    total_process_time = time.time() - total_start_time
    avg_infer_time = total_infer_time / processed_frames if processed_frames > 0 else 0.0
    avg_process_fps = processed_frames / total_process_time if total_process_time > 0 else 0.0

    print(f"\nâœ… ä¸²è¡Œè§†é¢‘å¤„ç†å®Œæˆï¼")
    print("\n" + "="*60)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡ç»“æœ")
    print("="*60)
    print(f"1. åŸºç¡€ä¿¡æ¯")
    print(f"   - æ€»å¤„ç†å¸§æ•°: {processed_frames} å¸§")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {total_process_time:.2f} ç§’")
    print(f"   - å¹³å‡å¤„ç†FPS: {avg_process_fps:.2f} fps")
    print(f"\n2. æ¨ç†æ€§èƒ½")
    print(f"   - æ€»æ¨ç†è€—æ—¶: {total_infer_time:.2f} ç§’")
    print(f"   - å•å¸§å¹³å‡æ¨ç†æ—¶é—´: {avg_infer_time:.3f} ç§’")
    print(f"\n3. è¾“å‡ºä¿¡æ¯")
    print(f"   - è¾“å‡ºè§†é¢‘è·¯å¾„: {video_save_path}")
    print(f"   - è¾“å‡ºè§†é¢‘åˆ†è¾¨ç‡: {stitched_resolution[0]}Ã—{stitched_resolution[1]}")
    print("="*60)


if __name__ == "__main__":
    main()
    main()
    main()