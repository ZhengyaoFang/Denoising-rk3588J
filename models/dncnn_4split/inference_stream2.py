import os
import cv2
import time
import random
import multiprocessing as mp
import numpy as np
import queue
from hailo_platform import (
    HEF,
    ConfigureParams,
    Device,
    FormatType,
    HailoSchedulingAlgorithm,
    HailoStreamInterface,
    InferVStreams,
    InputVStreamParams,
    OutputVStreamParams,
    VDevice,
)

# ---------------- å…¨å±€å‚æ•° ----------------
MAX_PENDING = 2        # æ¯ä¸ªè®¾å¤‡æœ€å¤šç¼“å†²ä»»åŠ¡æ•°
TARGET_FPS = 20        # ç›®æ ‡å¸§ç‡
HEF_PATH = "/home/firefly/Denoising-rk3588J/models/dncnn_4split/dncnn_4split_16pad.hef"
NUM_DEVICES = 2        # å¯ç”¨è®¾å¤‡æ•°é‡

# æ‘„åƒå¤´å‚æ•°
CAMERA_DEVICE_PATH = "/dev/video20"  # æ‘„åƒå¤´è®¾å¤‡è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
TARGET_RESOLUTION = (960, 720)       # ç›®æ ‡åˆ†è¾¨ç‡ (width, height)
TARGET_FPS = 20                      # ç›®æ ‡å¸§ç‡
VIDEO_FORMAT = cv2.VideoWriter_fourcc(*"MJPG")  # æ‘„åƒå¤´æ ¼å¼ï¼ˆMJPGæ”¯æŒé«˜å¸§ç‡ï¼‰


# -------------------------- è®¾å¤‡åˆå§‹åŒ–ï¼ˆå¤ç”¨åŸæ¨ç†é€»è¾‘ï¼‰ --------------------------
def init_device(hef_path, device_id):
    """åˆå§‹åŒ–å•ä¸ªHailoè®¾å¤‡å¹¶åŠ è½½æ¨¡å‹ï¼Œä¸åŸæ¨ç†ä»£ç é€»è¾‘ä¸€è‡´"""
    device_ids = Device.scan()
    if len(device_ids) <= device_id:
        raise RuntimeError(f"è®¾å¤‡ID {device_id} ä¸å­˜åœ¨ï¼Œä»…æ£€æµ‹åˆ° {len(device_ids)} ä¸ªè®¾å¤‡")
    
    print(f"åˆå§‹åŒ–è®¾å¤‡ {device_id}ï¼ˆç¡¬ä»¶ID: {device_ids[device_id]}ï¼‰...")
    
    # åˆ›å»ºè®¾å¤‡å‚æ•°ï¼ˆPCIeæ¥å£ï¼‰
    vdevice_params = VDevice.create_params()
    vdevice_params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE
    vdevice_params.device_ids.append(device_id)
    target = VDevice(params=vdevice_params)
    
    # åŠ è½½HEFæ¨¡å‹
    hef = HEF(hef_path)
    
    # é…ç½®ç½‘ç»œç»„
    configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)
    network_groups = target.configure(hef, configure_params)
    network_group = network_groups[0]
    network_group_params = network_group.create_params()
    
    # åˆ›å»ºè¾“å…¥/è¾“å‡ºæµå‚æ•°ï¼ˆè¾“å…¥float32ï¼Œè¾“å‡ºuint8ï¼‰
    input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)
    output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)
    
    # è·å–æµä¿¡æ¯
    input_vstream_info = hef.get_input_vstream_infos()[0]
    output_vstream_info = hef.get_output_vstream_infos()[0]
    
    device_info = {
        "target": target,
        "hef": hef,
        "network_group": network_group,
        "network_group_params": network_group_params,
        "input_vstreams_params": input_vstreams_params,
        "output_vstreams_params": output_vstreams_params,
        "input_vstream_info": input_vstream_info,
        "output_vstream_info": output_vstream_info,
        "device_id": device_id
    }
    
    print(f"è®¾å¤‡ {device_id} åˆå§‹åŒ–å®Œæˆ | è¾“å…¥å½¢çŠ¶: {input_vstream_info.shape} | è¾“å‡ºå½¢çŠ¶: {output_vstream_info.shape}")
    return device_info

def process_frame(frame):
    """
    å¤„ç†æ‘„åƒå¤´BGRå¸§ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆRGB+CHW+float32ï¼‰
    :param frame: cv2è¯»å–çš„BGRå¸§ï¼ˆHWCæ ¼å¼ï¼‰
    :return: é¢„å¤„ç†åçš„æ•°æ®ï¼ˆCHWæ ¼å¼ï¼‰ã€é¢„å¤„ç†è€—æ—¶ã€è°ƒæ•´ååŸå§‹BGRå¸§ï¼ˆç”¨äºæ‹¼æ¥ï¼‰
    """
    start_time = time.time()
    
    # 1. è°ƒæ•´åˆ†è¾¨ç‡ï¼ˆç¡®ä¿ä¸ç›®æ ‡ä¸€è‡´ï¼Œåç»­æ‹¼æ¥æ—¶å°ºå¯¸ç»Ÿä¸€ï¼‰
    frame_resized = cv2.resize(
        frame, 
        dsize=TARGET_RESOLUTION, 
        interpolation=cv2.INTER_LANCZOS4  # é«˜è´¨é‡æ’å€¼ï¼ˆä¸åŸä»£ç PIL.LANCZOSå¯¹åº”ï¼‰
    )
    
    # 2. BGRè½¬RGBï¼ˆcv2é»˜è®¤BGRï¼Œæ¨¡å‹éœ€è¦RGBï¼‰
    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

    # 3. æ ¼å¼è½¬æ¢ï¼šHWC -> CHWï¼Œ dtype -> float32
    # frame_chw = frame_rgb.transpose(2, 0, 1)  # (H,W,C) â†’ (C,H,W)
    frame_float = frame_rgb.astype(np.float32)
    
    process_time = time.time() - start_time
    return frame_float, process_time


# ---------------- å·¥ä½œè€…é€»è¾‘ ----------------
def device_worker(device_id, task_queue, queue2, result_queue, hef_path):
    """
    æ¯ä¸ªHailoè®¾å¤‡çš„æ¨ç†è¿›ç¨‹ï¼Œä¼˜å…ˆå¤„ç†è‡ªå·±çš„é˜Ÿåˆ—task_queueï¼Œ
    è‹¥ç©ºåˆ™å°è¯•ä»queue2ä¸­å·å–ä»»åŠ¡ã€‚
    """
    device = init_device(hef_path, device_id)
    print(f"[Device {device_id}] worker started, PID={os.getpid()}")
    while True:
        try:
            try:
                task = task_queue.get(timeout=0.05)
            except queue.Empty:
                try:
                    # ä»queue2å·å–ä»»åŠ¡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
                    task = queue2.get(timeout=0.05)
                except queue.Empty:
                    continue
            if task is None:
                break
            
            frame_id, frame = task
            frame = np.expand_dims(frame, axis=0)

            frame = split_and_stack(frame)
            output, infer_time = run_inference(device, frame)
            output = stack_to_original(output)
            result_queue.put((frame_id, output, infer_time))

        except Exception as e:
            print(f"[Device {device_id}] Error: {e}")
            continue
    device["target"].release()
    print(f"[Device {device_id}] exited.")

def run_inference(device, input_batch):
    """åœ¨å•ä¸ªè®¾å¤‡ä¸Šè¿è¡Œæ¨ç†ï¼Œè¿”å›æ¨ç†ç»“æœä¸è€—æ—¶"""
    network_group = device["network_group"]
    input_vstreams_params = device["input_vstreams_params"]
    output_vstreams_params = device["output_vstreams_params"]
    network_group_params = device["network_group_params"]
    input_vstream_info = device["input_vstream_info"]

    start_time = time.time()
    with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:
        with network_group.activate(network_group_params):
            input_data = {input_vstream_info.name: input_batch}
            infer_results = infer_pipeline.infer(input_data)
    
    inference_time = time.time() - start_time
    output_tensor = infer_results[device["output_vstream_info"].name]

    return output_tensor, inference_time

def split_and_stack(batch_tensor):
    """
    å°†å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„å›¾åƒæ•°ç»„åˆ†å‰²ä¸º4å¼ å­å›¾å¹¶å †å ä¸º[4, 360, 480, 3]
    
    å‚æ•°:
        batch_tensor: å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„numpyæ•°ç»„
        
    è¿”å›:
        å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„numpyæ•°ç»„
    """
    # æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    if batch_tensor.shape != (1, 720, 960, 3):
        raise ValueError(f"è¾“å…¥æ•°ç»„å½¢çŠ¶å¿…é¡»ä¸º[1, 720, 960, 3], å®é™…è¾“å…¥æ•°ç»„å½¢çŠ¶ä¸º{batch_tensor.shape}")
    
    # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œå¾—åˆ°[720, 960, 3]
    image = batch_tensor[0]
    
    # è®¡ç®—å­å›¾çš„é«˜åº¦å’Œå®½åº¦
    sub_height = 720 // 2
    sub_width = 960 // 2
    
    # åˆ†å‰²ä¸º4ä¸ªå­å›¾
    # å·¦ä¸Šè§’
    sub1 = image[:sub_height+16, :sub_width+16, :]
    # å·¦ä¸‹è§’
    sub2 = image[sub_height-16:, :sub_width+16, :]
    # å³ä¸Šè§’
    sub3 = image[:sub_height+16, sub_width-16:, :]
    # å³ä¸‹è§’
    sub4 = image[sub_height-16:, sub_width-16:, :]
    
    # å †å æˆ[4, 360, 480, 3]çš„æ•°ç»„
    stacked = np.stack([sub1, sub2, sub3, sub4], axis=0)
    
    return stacked

def stack_to_original(sub_images):
    """
    å°†å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„å­å›¾æ•°ç»„æ‹¼æ¥ä¸ºåŸå§‹å›¾åƒ[1, 720, 960, 3]
    
    å‚æ•°:
        sub_images: å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„numpyæ•°ç»„ï¼ŒåŒ…å«4ä¸ªå­å›¾
                    é¡ºåºåº”ä¸º[å·¦ä¸Š, å·¦ä¸‹, å³ä¸Š, å³ä¸‹]
        
    è¿”å›:
        å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„numpyæ•°ç»„ï¼ŒåŸå§‹å›¾åƒ
    """
    # æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    if sub_images.shape != (4, 376, 496, 3):
        raise ValueError(f"è¾“å…¥æ•°ç»„å½¢çŠ¶å¿…é¡»ä¸º[4, 360, 480, 3], å®é™…å½¢çŠ¶ä¸º{sub_images.shape}")
    
    # æå–4ä¸ªå­å›¾
    sub1, sub2, sub3, sub4 = sub_images[0], sub_images[1], sub_images[2], sub_images[3]
    
    # æ°´å¹³æ‹¼æ¥ç¬¬ä¸€è¡Œï¼ˆä¸ŠåŠéƒ¨åˆ†ï¼‰
    top_row = np.concatenate([sub1[:360,:480,:], sub3[:360,16:]], axis=1)
    
    # æ°´å¹³æ‹¼æ¥ç¬¬äºŒè¡Œï¼ˆä¸‹åŠéƒ¨åˆ†ï¼‰
    bottom_row = np.concatenate([sub2[16:,:480,:], sub4[16:, 16:,:]], axis=1)
    
    # å‚ç›´æ‹¼æ¥ä¸¤è¡Œï¼Œå¾—åˆ°å®Œæ•´å›¾åƒ
    full_image = np.concatenate([top_row, bottom_row], axis=0)
    
    # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º[1, 720, 960, 3]
    return np.expand_dims(full_image, axis=0)

# ---------------- ä¸»å¾ªç¯ ----------------
def main():
    cap = cv2.VideoCapture("/dev/video20")
    if not cap.isOpened():
        raise RuntimeError(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´è®¾å¤‡: {CAMERA_DEVICE_PATH}")
    
    # è®¾ç½®æ‘„åƒå¤´å‚æ•°ï¼ˆMJPGæ ¼å¼æ”¯æŒé«˜å¸§ç‡ï¼‰
    cap.set(cv2.CAP_PROP_FOURCC, VIDEO_FORMAT)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, TARGET_RESOLUTION[0])
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, TARGET_RESOLUTION[1])
    cap.set(cv2.CAP_PROP_FPS, TARGET_FPS)
    
    # éªŒè¯å‚æ•°æ˜¯å¦è®¾ç½®æˆåŠŸï¼ˆéƒ¨åˆ†æ‘„åƒå¤´å¯èƒ½ä¸æ”¯æŒç›®æ ‡é…ç½®ï¼‰
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    drop_prob = max(0, 1 - TARGET_FPS / actual_fps) if actual_fps > 0 else 0
    print(f"ğŸ¥ Target {TARGET_FPS} fps | Actual {actual_fps:.2f} fps | Drop Prob = {drop_prob:.2f}")

    # åˆå§‹åŒ–é˜Ÿåˆ—ä¸è¿›ç¨‹
    queues = [mp.Queue(MAX_PENDING) for _ in range(NUM_DEVICES)]  # q0/q1
    queue2 = mp.Queue()   # å¤‡ç”¨é˜Ÿåˆ—
    result_queue = mp.Queue()

    procs = []
    for i in range(NUM_DEVICES):
        p = mp.Process(target=device_worker, args=(i, queues[i], queue2, result_queue, HEF_PATH))
        p.start()
        procs.append(p)

    frame_id = 0
    last_show_time = 0
    next_display_id = 1
    result_buffer = {}

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # éšæœºä¸¢å¸§ï¼šæ§åˆ¶å¸§ç‡
            if random.random() < drop_prob:
                continue

            frame_id += 1
            processed_frame, _ = process_frame(frame)

            # åˆ†é…é˜Ÿåˆ—ï¼ˆè´Ÿè½½å‡è¡¡ + é™æµï¼‰
            assigned = False
            for q in queues:
                if q.qsize() < MAX_PENDING:
                    q.put((frame_id, processed_frame))
                    assigned = True
                    break
            if not assigned:
                queue2.put((frame_id, processed_frame))  # æ‰€æœ‰éƒ½æ»¡äº†æ”¾queue2
                

            # éé˜»å¡æ˜¾ç¤ºæœ€è¿‘çš„ç»“æœ
            try:
                while True:
                    fid, output_tensor, infer_time = result_queue.get(block=False)
                    frame_to_show = output_tensor[0]
                    if frame_to_show.dtype != np.uint8:
                        frame_to_show = np.clip(frame_to_show, 0, 255).astype(np.uint8)
                  
                    if next_display_id == fid:
                        cv2.imshow("Denoised Stream", frame_to_show)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            raise KeyboardInterrupt
                        next_display_id += 1
                    else:
                        result_buffer[fid] = frame_to_show
                    
                    while next_display_id in result_buffer:
                        frame_to_show = result_buffer.pop(next_display_id)
                        next_display_id += 1
                        cv2.imshow("Denoised Stream", frame_to_show)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            raise KeyboardInterrupt

            except queue.Empty:
                pass
            
            

    except KeyboardInterrupt:
        print("\nğŸ›‘ User interrupt, shutting down.")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        # å‘é€åœæ­¢ä¿¡å·
        for q in queues:
            q.put(None)
        for p in procs:
            p.join()
        print("âœ… All workers terminated.")

if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)
    main()
