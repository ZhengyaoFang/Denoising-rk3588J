import os
import time
import cv2
import numpy as np
import multiprocessing as mp
import queue
from tqdm import tqdm
import argparse
from hailo_platform import (
    HEF,
    ConfigureParams,
    Device,
    FormatType,
    HailoSchedulingAlgorithm,
    HailoStreamInterface,
    InferVStreams,
    InputVStreamParams,
    OutputVStreamParams,
    VDevice,
)

# -------------------------- æ ¸å¿ƒé…ç½®å‚æ•° --------------------------
# æ‘„åƒå¤´å‚æ•°
CAMERA_DEVICE_PATH = "/dev/video20"  # æ‘„åƒå¤´è®¾å¤‡è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
TARGET_RESOLUTION = (960, 720)       # ç›®æ ‡åˆ†è¾¨ç‡ (width, height)
TARGET_FPS = 20                      # ç›®æ ‡å¸§ç‡
VIDEO_FORMAT = cv2.VideoWriter_fourcc(*"MJPG")  # æ‘„åƒå¤´æ ¼å¼ï¼ˆMJPGæ”¯æŒé«˜å¸§ç‡ï¼‰

# æ¨ç†å‚æ•°
HEF_PATH = "/home/firefly/Denoising-rk3588J/demo/dncnn_4split_16pad.hef"  # Hailoæ¨¡å‹è·¯å¾„
BATCH_SIZE = 1                        # å•è®¾å¤‡æ‰¹æ¬¡å¤§å°ï¼ˆå¹³è¡¡å®æ—¶æ€§ä¸æ•ˆç‡ï¼‰
INPUT_SHAPE = (3, 720, 960)          # æ¨¡å‹è¾“å…¥å½¢çŠ¶ (channel, height, width)
NUM_DEVICES = 2                       # å¯ç”¨çš„HailoåŠ é€Ÿæ£’æ•°é‡
QUEUE_MAX_SIZE = 200                  # ä»»åŠ¡é˜Ÿåˆ—æœ€å¤§ç¼“å­˜ï¼ˆé¿å…å¸§å †ç§¯ï¼‰


# è§†é¢‘ä¿å­˜å‚æ•°ï¼ˆæ–°å¢ï¼‰
VIDEO_CODEC = cv2.VideoWriter_fourcc(*"mp4v")  # è¾“å‡ºè§†é¢‘ç¼–ç ï¼ˆmp4æ ¼å¼å…¼å®¹å¥½ï¼‰
VIDEO_EXT = ".mp4"                     # è§†é¢‘æ–‡ä»¶åç¼€
SPLIT_LINE_COLOR = (0, 255, 0)        # æ‹¼æ¥å¸§åˆ†å‰²çº¿é¢œè‰²ï¼ˆç»¿è‰²ï¼‰
SPLIT_LINE_WIDTH = 2                   # åˆ†å‰²çº¿å®½åº¦ï¼ˆåƒç´ ï¼‰
TEXT_COLOR = (255, 255, 255)          # æ–‡å­—é¢œè‰²ï¼ˆç™½è‰²ï¼‰
TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX   # æ–‡å­—å­—ä½“
TEXT_SIZE = 0.8                        # æ–‡å­—å¤§å°
TEXT_THICKNESS = 2                     # æ–‡å­—ç²—ç»†


# -------------------------- å¸§é¢„å¤„ç†ï¼ˆé€‚é…æ‘„åƒå¤´è¾“å…¥ï¼‰ --------------------------
def process_frame(frame):
    """
    å¤„ç†æ‘„åƒå¤´BGRå¸§ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆRGB+CHW+float32ï¼‰
    :param frame: cv2è¯»å–çš„BGRå¸§ï¼ˆHWCæ ¼å¼ï¼‰
    :return: é¢„å¤„ç†åçš„æ•°æ®ï¼ˆCHWæ ¼å¼ï¼‰ã€é¢„å¤„ç†è€—æ—¶ã€è°ƒæ•´ååŸå§‹BGRå¸§ï¼ˆç”¨äºæ‹¼æ¥ï¼‰
    """
    start_time = time.time()
    
    # 1. è°ƒæ•´åˆ†è¾¨ç‡ï¼ˆç¡®ä¿ä¸ç›®æ ‡ä¸€è‡´ï¼Œåç»­æ‹¼æ¥æ—¶å°ºå¯¸ç»Ÿä¸€ï¼‰
    frame_resized = cv2.resize(
        frame, 
        dsize=TARGET_RESOLUTION, 
        interpolation=cv2.INTER_LANCZOS4  # é«˜è´¨é‡æ’å€¼ï¼ˆä¸åŸä»£ç PIL.LANCZOSå¯¹åº”ï¼‰
    )
    
    # 2. BGRè½¬RGBï¼ˆcv2é»˜è®¤BGRï¼Œæ¨¡å‹éœ€è¦RGBï¼‰
    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

    # 3. æ ¼å¼è½¬æ¢ï¼šHWC -> CHWï¼Œ dtype -> float32
    # frame_chw = frame_rgb.transpose(2, 0, 1)  # (H,W,C) â†’ (C,H,W)
    frame_float = frame_rgb.astype(np.float32)
    
    process_time = time.time() - start_time
    return frame_float, process_time, frame_resized  # æ–°å¢è¿”å›è°ƒæ•´ååŸå§‹å¸§


# -------------------------- æ¨ç†ç»“æœåå¤„ç†ï¼ˆæ–°å¢ï¼‰ --------------------------
def postprocess_infer_result(infer_tensor):
    """
    å°†æ¨ç†è¾“å‡ºå¼ é‡è½¬æ¢ä¸ºBGRå¸§ï¼ˆé€‚é…OpenCVæ˜¾ç¤º/ä¿å­˜ï¼‰
    :param infer_tensor: æ¨ç†è¾“å‡ºå¼ é‡ï¼ˆCHWæ ¼å¼ï¼Œuint8ï¼‰
    :return: åå¤„ç†åçš„BGRå¸§ï¼ˆHWCæ ¼å¼ï¼‰
    """
    # 1. ç§»é™¤å¤šä½™ç»´åº¦ï¼ˆè‹¥æœ‰ï¼‰
    tensor_squeezed = np.squeeze(infer_tensor)
    
    # 2. CHW â†’ HWCï¼ˆOpenCVéœ€è¦HWCæ ¼å¼ï¼‰
    if tensor_squeezed.shape[0] in [3, 1]:  # è‹¥ä¸ºå•é€šé“/ä¸‰é€šé“CHWæ ¼å¼
        frame_hwc = tensor_squeezed.transpose(1, 2, 0)
    else:
        frame_hwc = tensor_squeezed  # è‹¥å·²ä¸ºHWCï¼Œç›´æ¥ä½¿ç”¨
    
    # 3. å•é€šé“ â†’ ä¸‰é€šé“ï¼ˆè‹¥æ¨¡å‹è¾“å‡ºä¸ºç°åº¦å›¾ï¼Œè½¬ä¸ºRGBå…¼å®¹æ ¼å¼ï¼‰
    if frame_hwc.shape[-1] == 1:
        frame_hwc = np.repeat(frame_hwc, 3, axis=-1)
    
    # 4. å°ºå¯¸è£å‰ªï¼ˆç¡®ä¿ä¸åŸå§‹å¸§åˆ†è¾¨ç‡ä¸€è‡´ï¼Œé¿å…æ‹¼æ¥é”™ä½ï¼‰
    frame_resized = cv2.resize(
        frame_hwc, 
        dsize=TARGET_RESOLUTION, 
        interpolation=cv2.INTER_LANCZOS4
    )
    
    # 5. æ ¼å¼è½¬æ¢ï¼šRGB â†’ BGRï¼ˆOpenCVé»˜è®¤BGRï¼‰
    frame_bgr = cv2.cvtColor(frame_resized.astype(np.uint8), cv2.COLOR_RGB2BGR)
    return frame_bgr

def split_and_stack(batch_tensor):
    """
    å°†å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„å›¾åƒæ•°ç»„åˆ†å‰²ä¸º4å¼ å­å›¾å¹¶å †å ä¸º[4, 360, 480, 3]
    
    å‚æ•°:
        batch_tensor: å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„numpyæ•°ç»„
        
    è¿”å›:
        å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„numpyæ•°ç»„
    """
    # æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    if batch_tensor.shape != (1, 720, 960, 3):
        raise ValueError("è¾“å…¥æ•°ç»„å½¢çŠ¶å¿…é¡»ä¸º[1, 720, 960, 3]")
    
    # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œå¾—åˆ°[720, 960, 3]
    image = batch_tensor[0]
    
    # è®¡ç®—å­å›¾çš„é«˜åº¦å’Œå®½åº¦
    sub_height = 720 // 2
    sub_width = 960 // 2
    
    # åˆ†å‰²ä¸º4ä¸ªå­å›¾
    # å·¦ä¸Šè§’
    sub1 = image[:sub_height+16, :sub_width+16, :]
    # å·¦ä¸‹è§’
    sub2 = image[sub_height-16:, :sub_width+16, :]
    # å³ä¸Šè§’
    sub3 = image[:sub_height+16, sub_width-16:, :]
    # å³ä¸‹è§’
    sub4 = image[sub_height-16:, sub_width-16:, :]
    
    # å †å æˆ[4, 360, 480, 3]çš„æ•°ç»„
    stacked = np.stack([sub1, sub2, sub3, sub4], axis=0)
    
    return stacked

def stack_to_original(sub_images):
    """
    å°†å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„å­å›¾æ•°ç»„æ‹¼æ¥ä¸ºåŸå§‹å›¾åƒ[1, 720, 960, 3]
    
    å‚æ•°:
        sub_images: å½¢çŠ¶ä¸º[4, 360, 480, 3]çš„numpyæ•°ç»„ï¼ŒåŒ…å«4ä¸ªå­å›¾
                    é¡ºåºåº”ä¸º[å·¦ä¸Š, å·¦ä¸‹, å³ä¸Š, å³ä¸‹]
        
    è¿”å›:
        å½¢çŠ¶ä¸º[1, 720, 960, 3]çš„numpyæ•°ç»„ï¼ŒåŸå§‹å›¾åƒ
    """
    # æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    if sub_images.shape != (4, 376, 496, 3):
        raise ValueError(f"è¾“å…¥æ•°ç»„å½¢çŠ¶å¿…é¡»ä¸º[4, 360, 480, 3], å®é™…å½¢çŠ¶ä¸º{sub_images.shape}")
    
    # æå–4ä¸ªå­å›¾
    sub1, sub2, sub3, sub4 = sub_images[0], sub_images[1], sub_images[2], sub_images[3]
    
    # æ°´å¹³æ‹¼æ¥ç¬¬ä¸€è¡Œï¼ˆä¸ŠåŠéƒ¨åˆ†ï¼‰
    top_row = np.concatenate([sub1[:360,:480,:], sub3[:360,16:]], axis=1)
    
    # æ°´å¹³æ‹¼æ¥ç¬¬äºŒè¡Œï¼ˆä¸‹åŠéƒ¨åˆ†ï¼‰
    bottom_row = np.concatenate([sub2[16:,:480,:], sub4[16:, 16:,:]], axis=1)
    
    # å‚ç›´æ‹¼æ¥ä¸¤è¡Œï¼Œå¾—åˆ°å®Œæ•´å›¾åƒ
    full_image = np.concatenate([top_row, bottom_row], axis=0)
    
    # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º[1, 720, 960, 3]
    return np.expand_dims(full_image, axis=0)

# -------------------------- å¸§æ‹¼æ¥ä¸æ ‡æ³¨ï¼ˆæ–°å¢ï¼‰ --------------------------
def stitch_frames(original_frame, infer_frame, fps):
    """
    æ¨ªå‘æ‹¼æ¥åŸå§‹å¸§ä¸æ¨ç†å¸§ï¼Œå¹¶æ·»åŠ æ ‡æ³¨ï¼ˆæ–‡å­—+åˆ†å‰²çº¿ï¼‰
    :param original_frame: è°ƒæ•´ååŸå§‹BGRå¸§ï¼ˆHWCï¼‰
    :param infer_frame: åå¤„ç†åçš„æ¨ç†BGRå¸§ï¼ˆHWCï¼‰
    :param fps: å½“å‰å®æ—¶FPSï¼ˆç”¨äºæ ‡æ³¨ï¼‰
    :return: æ‹¼æ¥åçš„BGRå¸§ï¼ˆHWCï¼‰
    """
    # 1. æ¨ªå‘æ‹¼æ¥ä¸¤å¸§ï¼ˆå®½åº¦å åŠ ï¼Œé«˜åº¦ä¸€è‡´ï¼‰
    stitched_frame = cv2.hconcat([original_frame, infer_frame])
    
    # 2. æ·»åŠ åˆ†å‰²çº¿ï¼ˆåŒºåˆ†åŸå§‹å¸§ä¸æ¨ç†å¸§ï¼‰
    split_x = original_frame.shape[1]  # åˆ†å‰²çº¿Xåæ ‡ï¼ˆåŸå§‹å¸§å®½åº¦å¤„ï¼‰
    cv2.line(
        stitched_frame, 
        (split_x, 0),  # èµ·ç‚¹ï¼ˆåˆ†å‰²çº¿é¡¶éƒ¨ï¼‰
        (split_x, stitched_frame.shape[0]),  # ç»ˆç‚¹ï¼ˆåˆ†å‰²çº¿åº•éƒ¨ï¼‰
        SPLIT_LINE_COLOR, 
        SPLIT_LINE_WIDTH
    )
    
    # 3. æ·»åŠ æ–‡å­—æ ‡æ³¨ï¼ˆå·¦ä¸Šè§’ï¼šåŸå§‹å¸§æ ‡è¯†ï¼Œå³ä¸Šè§’ï¼šæ¨ç†å¸§æ ‡è¯†ï¼Œå³ä¸‹è§’ï¼šFPSï¼‰
    # 3.1 åŸå§‹å¸§æ ‡è¯†
    cv2.putText(
        stitched_frame, 
        "Original Frame", 
        (20, 40),  # æ–‡å­—ä½ç½®ï¼ˆå·¦ä¸Šè§’åç§»ï¼‰
        TEXT_FONT, 
        TEXT_SIZE, 
        TEXT_COLOR, 
        TEXT_THICKNESS
    )
    
    # 3.2 æ¨ç†å¸§æ ‡è¯†
    cv2.putText(
        stitched_frame, 
        "Inferred Frame", 
        (split_x + 20, 40),  # æ¨ç†å¸§åŒºåŸŸå·¦ä¸Šè§’åç§»
        TEXT_FONT, 
        TEXT_SIZE, 
        TEXT_COLOR, 
        TEXT_THICKNESS
    )    
    return stitched_frame


# -------------------------- è®¾å¤‡åˆå§‹åŒ–ï¼ˆå¤ç”¨åŸæ¨ç†é€»è¾‘ï¼‰ --------------------------
def init_device(hef_path, device_id):
    """åˆå§‹åŒ–å•ä¸ªHailoè®¾å¤‡å¹¶åŠ è½½æ¨¡å‹ï¼Œä¸åŸæ¨ç†ä»£ç é€»è¾‘ä¸€è‡´"""
    device_ids = Device.scan()
    if len(device_ids) <= device_id:
        raise RuntimeError(f"è®¾å¤‡ID {device_id} ä¸å­˜åœ¨ï¼Œä»…æ£€æµ‹åˆ° {len(device_ids)} ä¸ªè®¾å¤‡")
    
    print(f"åˆå§‹åŒ–è®¾å¤‡ {device_id}ï¼ˆç¡¬ä»¶ID: {device_ids[device_id]}ï¼‰...")
    
    # åˆ›å»ºè®¾å¤‡å‚æ•°ï¼ˆPCIeæ¥å£ï¼‰
    vdevice_params = VDevice.create_params()
    vdevice_params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE
    vdevice_params.device_ids.append(device_id)
    target = VDevice(params=vdevice_params)
    
    # åŠ è½½HEFæ¨¡å‹
    hef = HEF(hef_path)
    
    # é…ç½®ç½‘ç»œç»„
    configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)
    network_groups = target.configure(hef, configure_params)
    network_group = network_groups[0]
    network_group_params = network_group.create_params()
    
    # åˆ›å»ºè¾“å…¥/è¾“å‡ºæµå‚æ•°ï¼ˆè¾“å…¥float32ï¼Œè¾“å‡ºuint8ï¼‰
    input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)
    output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)
    
    # è·å–æµä¿¡æ¯
    input_vstream_info = hef.get_input_vstream_infos()[0]
    output_vstream_info = hef.get_output_vstream_infos()[0]
    
    device_info = {
        "target": target,
        "hef": hef,
        "network_group": network_group,
        "network_group_params": network_group_params,
        "input_vstreams_params": input_vstreams_params,
        "output_vstreams_params": output_vstreams_params,
        "input_vstream_info": input_vstream_info,
        "output_vstream_info": output_vstream_info,
        "device_id": device_id
    }
    
    print(f"è®¾å¤‡ {device_id} åˆå§‹åŒ–å®Œæˆ | è¾“å…¥å½¢çŠ¶: {input_vstream_info.shape} | è¾“å‡ºå½¢çŠ¶: {output_vstream_info.shape}")
    return device_info

# -------------------------- æ¨ç†å‡½æ•°ï¼ˆå¤ç”¨åŸé€»è¾‘ï¼‰ --------------------------
def run_inference(device, input_batch):
    """åœ¨å•ä¸ªè®¾å¤‡ä¸Šè¿è¡Œæ¨ç†ï¼Œè¿”å›æ¨ç†ç»“æœä¸è€—æ—¶"""
    network_group = device["network_group"]
    input_vstreams_params = device["input_vstreams_params"]
    output_vstreams_params = device["output_vstreams_params"]
    network_group_params = device["network_group_params"]
    input_vstream_info = device["input_vstream_info"]
    
    start_time = time.time()
    with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:
        with network_group.activate(network_group_params):
            input_data = {input_vstream_info.name: input_batch}
            infer_results = infer_pipeline.infer(input_data)
    
    inference_time = time.time() - start_time
    output_tensor = infer_results[device["output_vstream_info"].name]
    return output_tensor, inference_time


# -------------------------- å·¥ä½œè¿›ç¨‹ï¼ˆä¿®æ”¹ï¼šä¿ç•™æ¨ç†ç»“æœç”¨äºæ‹¼æ¥ï¼‰ --------------------------
def worker_process(device_id, task_queue, result_queue, hef_path):
    """
    è®¾å¤‡å·¥ä½œè¿›ç¨‹ï¼šæ¥æ”¶æ‘„åƒå¤´å¸§æ‰¹æ¬¡ â†’ æ¨ç† â†’ è¿”å›ç»“æœï¼ˆå«æ¨ç†å¼ é‡ï¼‰
    :param task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå…ƒç´ ï¼š(batch_tensor, actual_batch_size, batch_index, original_frames)ï¼‰
    :param result_queue: ç»“æœé˜Ÿåˆ—ï¼ˆå…ƒç´ ï¼š(batch_index, actual_batch_size, infer_time, infer_tensors)ï¼‰
    """
    try:
        device = init_device(hef_path, device_id)
        print(f"è®¾å¤‡ {device_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ï¼ˆPID: {os.getpid()}ï¼‰")
        
        while True:
            task = task_queue.get()
            if task is None:  # ç»ˆæ­¢ä¿¡å·
                break
            
            batch_tensor, actual_batch_size, batch_index, _ = task  # å¿½ç•¥åŸå§‹å¸§ï¼ˆä¸»è¿›ç¨‹ä¿ç•™ï¼‰
            batch_tensor = split_and_stack(batch_tensor)
            # æ‰§è¡Œæ¨ç†ï¼ˆä¿ç•™è¾“å‡ºå¼ é‡ï¼Œç”¨äºåå¤„ç†ï¼‰
            infer_tensors, infer_time = run_inference(device, batch_tensor)
            infer_tensors = stack_to_original(infer_tensors)
            # å‘ä¸»è¿›ç¨‹è¿”å›ï¼šæ‰¹æ¬¡ç´¢å¼•ã€æœ‰æ•ˆå¸§æ•°ã€æ¨ç†è€—æ—¶ã€æ¨ç†å¼ é‡ï¼ˆä»…è¿”å›æœ‰æ•ˆéƒ¨åˆ†ï¼‰
            result_queue.put((batch_index, actual_batch_size, infer_time, infer_tensors[:actual_batch_size]))
    
    except Exception as e:
        print(f"è®¾å¤‡ {device_id} å·¥ä½œè¿›ç¨‹å‡ºé”™: {str(e)}")
    finally:
        # é‡Šæ”¾è®¾å¤‡èµ„æº
        if "device" in locals():
            device["target"].release()
        print(f"è®¾å¤‡ {device_id} å·¥ä½œè¿›ç¨‹é€€å‡º")

def parse_args():
    parser = argparse.ArgumentParser(description="Video inference settings")
    parser.add_argument(
        "--camera",
        type=str,
        default="/dev/video20",
        help="Camera id."
    )
    parser.add_argument(
        "--save_dir",
        type=str,
        default="output/inference_videos",
        help="Directory to save inference results (default: output/inference_videos)"
    )
    parser.add_argument(
        "--seconds",
        type=int,
        default=10,
    )
    
    args = parser.parse_args()
    return args

# -------------------------- ä¸»æµç¨‹ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šå¸§ç¼“å­˜+æ‹¼æ¥+è§†é¢‘ä¿å­˜ï¼‰ --------------------------
def main():
    args = parse_args()
    RUN_DURATION = args.seconds
    SAVE_VIDEO_DIR = args.save_dir
    total_start_time = time.time()
    print("="*60)
    print(f"å¼€å§‹å®æ—¶æ¨ç†æµ‹è¯• | ç›®æ ‡: {TARGET_RESOLUTION[0]}Ã—{TARGET_RESOLUTION[1]} @ {TARGET_FPS}fps | è¿è¡Œæ—¶é•¿: {RUN_DURATION}s")
    print(f"è®¾å¤‡æ•°é‡: {NUM_DEVICES} | æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} | æ¨¡å‹è·¯å¾„: {HEF_PATH}")
    print(f"è§†é¢‘ä¿å­˜ç›®å½•: {SAVE_VIDEO_DIR} | è§†é¢‘ç¼–ç : {VIDEO_CODEC.to_bytes(4, 'little').decode('utf-8')}")
    print("="*60)

    # -------------------------- 1. åˆå§‹åŒ–è§†é¢‘ä¿å­˜ç›®å½•ä¸å¯¹è±¡ï¼ˆæ–°å¢ï¼‰ --------------------------
    os.makedirs(SAVE_VIDEO_DIR, exist_ok=True)
    # ç”Ÿæˆå”¯ä¸€è§†é¢‘æ–‡ä»¶åï¼ˆæ—¶é—´æˆ³+åˆ†è¾¨ç‡+å¸§ç‡ï¼‰
    video_filename = f"infer_stitched_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}_{TARGET_FPS}fps{VIDEO_EXT}"
    video_save_path = os.path.join(SAVE_VIDEO_DIR, video_filename)
    # è®¡ç®—æ‹¼æ¥åè§†é¢‘åˆ†è¾¨ç‡ï¼ˆå®½åº¦=2*åŸå§‹å®½åº¦ï¼Œé«˜åº¦=åŸå§‹é«˜åº¦ï¼‰
    stitched_resolution = (TARGET_RESOLUTION[0] * 2, TARGET_RESOLUTION[1])
    # åˆå§‹åŒ–è§†é¢‘å†™å…¥å¯¹è±¡
    video_writer = cv2.VideoWriter(
        video_save_path,
        VIDEO_CODEC,
        TARGET_FPS,  # è§†é¢‘å¸§ç‡ï¼ˆä¸ç›®æ ‡ä¸€è‡´ï¼‰
        stitched_resolution  # æ‹¼æ¥ååˆ†è¾¨ç‡
    )
    if not video_writer.isOpened():
        raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–è§†é¢‘å†™å…¥å¯¹è±¡ï¼Œè·¯å¾„ï¼š{video_save_path}")
    print(f"\nâœ… è§†é¢‘å†™å…¥å¯¹è±¡å·²åˆå§‹åŒ– | ä¿å­˜è·¯å¾„: {video_save_path} | æ‹¼æ¥åˆ†è¾¨ç‡: {stitched_resolution[0]}Ã—{stitched_resolution[1]}")

    # -------------------------- 2. åˆå§‹åŒ–è¿›ç¨‹ä¸é˜Ÿåˆ— --------------------------
    # ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¯ä¸ªè®¾å¤‡ä¸€ä¸ªï¼‰ï¼šæ–°å¢åŸå§‹å¸§ç¼“å­˜ï¼ˆç”¨äºæ‹¼æ¥ï¼‰
    task_queues = [mp.Queue(maxsize=QUEUE_MAX_SIZE) for _ in range(NUM_DEVICES)]
    # ç»“æœé˜Ÿåˆ—ï¼šæ¥æ”¶æ¨ç†ç»Ÿè®¡+æ¨ç†å¼ é‡
    result_queue = mp.Queue()
    
    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
    processes = []
    for device_id in range(NUM_DEVICES):
        p = mp.Process(
            target=worker_process,
            args=(device_id, task_queues[device_id], result_queue, HEF_PATH)
        )
        p.start()
        processes.append(p)
    time.sleep(2)  # ç­‰å¾…è®¾å¤‡åˆå§‹åŒ–å®Œæˆ

    # -------------------------- 3. åˆå§‹åŒ–æ‘„åƒå¤´ --------------------------
    cap = cv2.VideoCapture(CAMERA_DEVICE_PATH)
    if not cap.isOpened():
        raise RuntimeError(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´è®¾å¤‡: {CAMERA_DEVICE_PATH}")
    
    # è®¾ç½®æ‘„åƒå¤´å‚æ•°ï¼ˆMJPGæ ¼å¼æ”¯æŒé«˜å¸§ç‡ï¼‰
    cap.set(cv2.CAP_PROP_FOURCC, VIDEO_FORMAT)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, TARGET_RESOLUTION[0])
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, TARGET_RESOLUTION[1])
    cap.set(cv2.CAP_PROP_FPS, TARGET_FPS)
    
    # éªŒè¯å‚æ•°æ˜¯å¦è®¾ç½®æˆåŠŸï¼ˆéƒ¨åˆ†æ‘„åƒå¤´å¯èƒ½ä¸æ”¯æŒç›®æ ‡é…ç½®ï¼‰
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"\næ‘„åƒå¤´å‚æ•°éªŒè¯ | å®é™…åˆ†è¾¨ç‡: {actual_width}Ã—{actual_height} | å®é™…å¸§ç‡: {actual_fps:.1f}fps")
    if (actual_width, actual_height) != TARGET_RESOLUTION:
        print(f"âš ï¸  æ‘„åƒå¤´ä¸æ”¯æŒ {TARGET_RESOLUTION[0]}Ã—{TARGET_RESOLUTION[1]}ï¼Œå°†ä½¿ç”¨å®é™…åˆ†è¾¨ç‡ {actual_width}Ã—{actual_height}")
        # æ›´æ–°æ‹¼æ¥åˆ†è¾¨ç‡ï¼ˆè‹¥å®é™…åˆ†è¾¨ç‡ä¸ç›®æ ‡ä¸ä¸€è‡´ï¼‰
        stitched_resolution = (actual_width * 2, actual_height)
        video_writer.set(cv2.CAP_PROP_FRAME_WIDTH, stitched_resolution[0])
        video_writer.set(cv2.CAP_PROP_FRAME_HEIGHT, stitched_resolution[1])

    # -------------------------- 4. åˆå§‹åŒ–ç»Ÿè®¡ä¸ç¼“å­˜å˜é‡ï¼ˆæ–°å¢å¸§ç¼“å­˜ï¼‰ --------------------------
    read_total_frames = 0          # æ‘„åƒå¤´è¯»å–çš„æ€»å¸§æ•°
    infer_total_frames = 0         # æˆåŠŸæ¨ç†çš„æ€»å¸§æ•°
    saved_total_frames = 0         # æˆåŠŸä¿å­˜çš„æ‹¼æ¥å¸§æ•°
    read_start_time = time.time()  # æ‘„åƒå¤´è¯»å–å¼€å§‹æ—¶é—´
    infer_start_time = None        # æ¨ç†å¼€å§‹æ—¶é—´ï¼ˆç¬¬ä¸€ä¸ªæ‰¹æ¬¡å‘é€æ—¶è®°å½•ï¼‰
    total_infer_compute_time = 0   # æ¨ç†è®¡ç®—æ€»è€—æ—¶ï¼ˆæ‰€æœ‰è®¾å¤‡ç´¯åŠ ï¼‰
    batch_index = 0                # æ‰¹æ¬¡ç´¢å¼•ï¼ˆç”¨äºåŒ¹é…ä»»åŠ¡ä¸ç»“æœï¼‰
    
    # è®¾å¤‡å¸§ç¼“å­˜ï¼šæ–°å¢åŸå§‹å¸§ç¼“å­˜ï¼ˆæ¯ä¸ªæ‰¹æ¬¡å¯¹åº”ä¸€ç»„åŸå§‹å¸§ï¼Œç”¨äºæ‹¼æ¥ï¼‰
    device_buffers = {
        "processed_frames": [[] for _ in range(NUM_DEVICES)],  # é¢„å¤„ç†åå¸§ï¼ˆç”¨äºæ¨ç†ï¼‰
        "original_frames": [[] for _ in range(NUM_DEVICES)],    # è°ƒæ•´ååŸå§‹å¸§ï¼ˆç”¨äºæ‹¼æ¥ï¼‰
    }
    
    # ç»“æœç¼“å­˜ï¼šåŒ¹é…æ‰¹æ¬¡ç´¢å¼•ä¸æ¨ç†ç»“æœï¼ˆè§£å†³å¤šè®¾å¤‡è¿”å›é¡ºåºä¹±åºé—®é¢˜ï¼‰
    result_cache = {}

    # -------------------------- 5. å®æ—¶è¯»å–+æ¨ç†+æ‹¼æ¥+ä¿å­˜å¾ªç¯ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ --------------------------
    print(f"\nå¼€å§‹è¯»å–æ‘„åƒå¤´å¸§ï¼ˆæŒ‰ 'q' æå‰é€€å‡ºï¼‰...")
    while (time.time() - read_start_time) < RUN_DURATION:
        # è¯»å–æ‘„åƒå¤´å¸§
        ret, frame = cap.read()
        if not ret:
            print("âš ï¸  æ— æ³•è¯»å–æ‘„åƒå¤´å¸§ï¼Œé€€å‡ºå¾ªç¯")
            break
        
        read_total_frames += 1
        current_time = time.time()

        # é¢„å¤„ç†å¸§ï¼ˆæ–°å¢è¿”å›è°ƒæ•´ååŸå§‹å¸§ï¼‰
        frame_processed, _, frame_original = process_frame(frame)

        # åˆ†é…å¸§åˆ°è®¾å¤‡ç¼“å­˜ï¼ˆè½®è¯¢åˆ†é…ï¼Œå‡è¡¡è´Ÿè½½ï¼‰
        target_device_id = read_total_frames % NUM_DEVICES
        device_buffers["processed_frames"][target_device_id].append(frame_processed)
        device_buffers["original_frames"][target_device_id].append(frame_original)  # ç¼“å­˜åŸå§‹å¸§

        # å½“ç¼“å­˜è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå‘é€æ¨ç†ä»»åŠ¡ï¼ˆæ–°å¢åŸå§‹å¸§åˆ—è¡¨ï¼‰
        if len(device_buffers["processed_frames"][target_device_id]) >= BATCH_SIZE:
            # æå–æ‰¹æ¬¡å¸§å¹¶è¡¥é›¶ï¼ˆä¸è¶³æ‰¹æ¬¡å¤§å°æ—¶ï¼‰
            batch_processed = device_buffers["processed_frames"][target_device_id][:BATCH_SIZE]
            batch_original = device_buffers["original_frames"][target_device_id][:BATCH_SIZE]  # åŸå§‹å¸§æ‰¹æ¬¡
            actual_batch_size = len(batch_processed)
            
            if actual_batch_size < BATCH_SIZE:
                pad_size = BATCH_SIZE - actual_batch_size
                batch_processed += [np.zeros_like(batch_processed[0]) for _ in range(pad_size)]
            
            # è½¬æ¢ä¸ºæ‰¹æ¬¡å¼ é‡
            batch_tensor = np.stack(batch_processed, axis=0)

            # å‘é€ä»»åŠ¡åˆ°é˜Ÿåˆ—ï¼ˆåŒ…å«åŸå§‹å¸§æ‰¹æ¬¡ï¼Œéé˜»å¡é¿å…é˜»å¡è¯»å–ï¼‰
            try:
                task_queues[target_device_id].put(
                    (batch_tensor, actual_batch_size, batch_index, batch_original),
                    block=False
                )
                # ç¼“å­˜åŸå§‹å¸§æ‰¹æ¬¡ï¼ˆä¸»è¿›ç¨‹ä¿ç•™ï¼Œç”¨äºåç»­æ‹¼æ¥ï¼‰
                result_cache[batch_index] = {
                    "original_frames": batch_original,
                    "processed": False  # æ ‡è®°æ˜¯å¦å·²å¤„ç†æ¨ç†ç»“æœ
                }
                batch_index += 1
                # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´ï¼ˆç¬¬ä¸€ä¸ªä»»åŠ¡å‘é€æ—¶ï¼‰
                if infer_start_time is None:
                    infer_start_time = current_time
                print(f"ğŸ“¤ è®¾å¤‡ {target_device_id} å‘é€æ‰¹æ¬¡ {batch_index-1}ï¼ˆæœ‰æ•ˆå¸§: {actual_batch_size}ï¼‰", end="\r")
            except mp.Queue.Full:
                print(f"âš ï¸  è®¾å¤‡ {target_device_id} ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒå½“å‰æ‰¹æ¬¡", end="\r")
                # ä¸¢å¼ƒå¯¹åº”çš„åŸå§‹å¸§ç¼“å­˜
                device_buffers["original_frames"][target_device_id] = device_buffers["original_frames"][target_device_id][BATCH_SIZE:]

            # æ¸…ç©ºå·²å‘é€çš„ç¼“å­˜
            device_buffers["processed_frames"][target_device_id] = device_buffers["processed_frames"][target_device_id][BATCH_SIZE:]
            device_buffers["original_frames"][target_device_id] = device_buffers["original_frames"][target_device_id][BATCH_SIZE:]

        # å¤„ç†æ¨ç†ç»“æœï¼ˆéé˜»å¡ï¼Œé¿å…é˜»å¡è¯»å–ï¼‰
        while not result_queue.empty():
            try:
                # ä»ç»“æœé˜Ÿåˆ—è·å–æ•°æ®ï¼ˆæ‰¹æ¬¡ç´¢å¼•ã€æœ‰æ•ˆå¸§æ•°ã€æ¨ç†è€—æ—¶ã€æ¨ç†å¼ é‡ï¼‰
                batch_idx, actual_frames, infer_time, infer_tensors = result_queue.get(block=False)
                total_infer_compute_time += infer_time
                infer_total_frames += actual_frames
                
                # æ£€æŸ¥è¯¥æ‰¹æ¬¡åŸå§‹å¸§æ˜¯å¦åœ¨ç¼“å­˜ä¸­
                if batch_idx not in result_cache or result_cache[batch_idx]["processed"]:
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} åŸå§‹å¸§ç¼“å­˜ä¸¢å¤±æˆ–å·²å¤„ç†ï¼Œè·³è¿‡æ‹¼æ¥")
                    continue
                
                # è·å–è¯¥æ‰¹æ¬¡åŸå§‹å¸§
                batch_original = result_cache[batch_idx]["original_frames"]
                
                # é€å¸§å¤„ç†ï¼šåå¤„ç†æ¨ç†ç»“æœ â†’ æ‹¼æ¥ â†’ ä¿å­˜
                for i in range(actual_frames):
                    # åå¤„ç†æ¨ç†å¼ é‡ä¸ºBGRå¸§
                    infer_frame = postprocess_infer_result(infer_tensors[i])
                    # è®¡ç®—å®æ—¶FPSï¼ˆç”¨äºæ ‡æ³¨ï¼‰
                    elapsed_time = time.time() - read_start_time
                    current_fps = read_total_frames / elapsed_time if elapsed_time > 1e-3 else 0.0
                    # æ‹¼æ¥åŸå§‹å¸§ä¸æ¨ç†å¸§
                    stitched_frame = stitch_frames(batch_original[i], infer_frame, current_fps)
                    # å†™å…¥è§†é¢‘
                    video_writer.write(stitched_frame)
                    saved_total_frames += 1
                
                # æ ‡è®°è¯¥æ‰¹æ¬¡å·²å¤„ç†ï¼Œé‡Šæ”¾ç¼“å­˜
                result_cache[batch_idx]["processed"] = True
                del result_cache[batch_idx]
                
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ¨ç†ç»“æœæ—¶å‡ºé”™: {str(e)}", end="\r")

        # æŒ‰ 'q' é”®æå‰é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord("q"):
            print("\nğŸ›‘ ç”¨æˆ·æŒ‰ä¸‹ 'q' é”®ï¼Œæå‰é€€å‡º")
            break

    # -------------------------- 6. å¤„ç†å‰©ä½™å¸§ï¼ˆç¼“å­˜ä¸­æœªå‘é€çš„å¸§ï¼‰ --------------------------
    print(f"\n\nå¤„ç†ç¼“å­˜ä¸­å‰©ä½™çš„å¸§...")
    for device_id in range(NUM_DEVICES):
        remaining_processed = device_buffers["processed_frames"][device_id]
        remaining_original = device_buffers["original_frames"][device_id]
        if len(remaining_processed) == 0:
            continue
        
        # å¤„ç†å‰©ä½™å¸§ï¼ˆä¸è¶³æ‰¹æ¬¡å¤§å°æ—¶è¡¥é›¶ï¼‰
        actual_batch_size = len(remaining_processed)
        if actual_batch_size < BATCH_SIZE:
            pad_size = BATCH_SIZE - actual_batch_size
            remaining_processed += [np.zeros_like(remaining_processed[0]) for _ in range(pad_size)]
        
        batch_tensor = np.stack(remaining_processed, axis=0)
        try:
            # å‘é€å‰©ä½™ä»»åŠ¡ï¼ˆåŒ…å«åŸå§‹å¸§ï¼‰
            task_queues[device_id].put(
                (batch_tensor, actual_batch_size, batch_index, remaining_original),
                block=True, 
                timeout=5
            )
            # ç¼“å­˜åŸå§‹å¸§
            result_cache[batch_index] = {
                "original_frames": remaining_original,
                "processed": False
            }
            batch_index += 1
            print(f"ğŸ“¤ è®¾å¤‡ {device_id} å‘é€å‰©ä½™æ‰¹æ¬¡ {batch_index-1}ï¼ˆæœ‰æ•ˆå¸§: {actual_batch_size}ï¼‰")
        except (mp.Queue.Full, TimeoutError):
            print(f"âš ï¸  è®¾å¤‡ {device_id} é˜Ÿåˆ—æ»¡/è¶…æ—¶ï¼Œæ— æ³•å‘é€å‰©ä½™ {actual_batch_size} å¸§")

    # -------------------------- 7. å¤„ç†å‰©ä½™æ¨ç†ç»“æœï¼ˆç¡®ä¿æ‰€æœ‰å¸§éƒ½è¢«æ‹¼æ¥ï¼‰ --------------------------
    print(f"\nå¤„ç†å‰©ä½™æ¨ç†ç»“æœ...")
    remaining_batches = len(result_cache)
    if remaining_batches > 0:
        print(f"ç­‰å¾… {remaining_batches} ä¸ªæ‰¹æ¬¡çš„æ¨ç†ç»“æœ...")
        start_wait_time = time.time()
        # ç­‰å¾…å‰©ä½™ç»“æœï¼ˆè¶…æ—¶æ—¶é—´10ç§’ï¼‰
        while len(result_cache) > 0 and (time.time() - start_wait_time) < 10:
            if not result_queue.empty():
                try:
                    batch_idx, actual_frames, infer_time, infer_tensors = result_queue.get(block=False)
                    total_infer_compute_time += infer_time
                    infer_total_frames += actual_frames
                    
                    if batch_idx not in result_cache or result_cache[batch_idx]["processed"]:
                        continue
                    
                    # æ‹¼æ¥å¹¶ä¿å­˜å‰©ä½™å¸§
                    batch_original = result_cache[batch_idx]["original_frames"]
                    for i in range(actual_frames):
                        infer_frame = postprocess_infer_result(infer_tensors[i])
                        elapsed_time = time.time() - read_start_time
                        current_fps = read_total_frames / elapsed_time if elapsed_time > 1e-3 else 0.0
                        stitched_frame = stitch_frames(batch_original[i], infer_frame, current_fps)
                        video_writer.write(stitched_frame)
                        saved_total_frames += 1
                    
                    result_cache[batch_idx]["processed"] = True
                    del result_cache[batch_idx]
                    print(f"âœ… å¤„ç†å‰©ä½™æ‰¹æ¬¡ {batch_idx}ï¼Œè¿˜å‰© {len(result_cache)} ä¸ªæ‰¹æ¬¡", end="\r")
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†å‰©ä½™ç»“æœå‡ºé”™: {str(e)}", end="\r")
            time.sleep(0.1)  # é¿å…CPUç©ºè½¬

    # -------------------------- 8. å‘é€ç»ˆæ­¢ä¿¡å·å¹¶æ¸…ç† --------------------------
    # å‘æ‰€æœ‰å·¥ä½œè¿›ç¨‹å‘é€ç»ˆæ­¢ä¿¡å·
    for q in task_queues:
        q.put(None)
    
    # æ”¶é›†å‰©ä½™ç»“æœï¼ˆä»…ç»Ÿè®¡ï¼Œä¸ä¿å­˜ï¼‰
    print(f"\n\næ”¶é›†å‰©ä½™æ¨ç†ç»Ÿè®¡...")
    processed_batches = 0
    total_batches = batch_index
    progress_bar = tqdm(total=total_batches, desc="æ¨ç†è¿›åº¦")
    while processed_batches < total_batches:
        try:
            batch_idx, actual_frames, infer_time, _ = result_queue.get(block=True, timeout=5)
            total_infer_compute_time += infer_time
            infer_total_frames += actual_frames
            processed_batches += 1
            progress_bar.update(1)
        except queue.Empty:
            print(f"âš ï¸  ç»“æœé˜Ÿåˆ—è¶…æ—¶ï¼Œæœªæ”¶é›†åˆ°æ‰€æœ‰æ‰¹æ¬¡ç»“æœï¼ˆå·²å¤„ç† {processed_batches}/{total_batches}ï¼‰")
            break
    progress_bar.close()

    # -------------------------- 9. é‡Šæ”¾èµ„æºï¼ˆå…³é”®ï¼šå…³é—­è§†é¢‘å†™å…¥å¯¹è±¡ï¼‰ --------------------------
    video_writer.release()  # å¿…é¡»å…³é—­ï¼Œå¦åˆ™è§†é¢‘æ–‡ä»¶æŸå
    cap.release()
    cv2.destroyAllWindows()
    for p in processes:
        p.join(timeout=10)
        print(f"ğŸ”š è®¾å¤‡è¿›ç¨‹ {p.pid} é€€å‡ºçŠ¶æ€: {'æ­£å¸¸' if p.exitcode == 0 else 'å¼‚å¸¸'}")
    print(f"\nâœ… è§†é¢‘å·²ä¿å­˜è‡³: {video_save_path} | å…±ä¿å­˜ {saved_total_frames} å¸§æ‹¼æ¥ç”»é¢")

    # -------------------------- 10. è®¡ç®—å¹¶è¾“å‡ºFPSç»Ÿè®¡ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰ --------------------------
    # 3. è§†é¢‘ä¿å­˜æ€§èƒ½ï¼ˆæ–°å¢ï¼‰
    print(f"\n3. è§†é¢‘ä¿å­˜æ€§èƒ½")
    print(f"   - æ€»ä¿å­˜æ‹¼æ¥å¸§æ•°: {saved_total_frames} å¸§")
    print(f"   - è§†é¢‘ä¿å­˜è·¯å¾„: {video_save_path}")
    print(f"   - è§†é¢‘åˆ†è¾¨ç‡: {stitched_resolution[0]}Ã—{stitched_resolution[1]}")
    print(f"   - è§†é¢‘ç›®æ ‡å¸§ç‡: {TARGET_FPS} fps")


if __name__ == "__main__":
    from datetime import datetime  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªä½¿ç”¨æ—¶åŠ è½½
    # Windowsç³»ç»Ÿéœ€å¼ºåˆ¶ä½¿ç”¨spawnå¯åŠ¨æ–¹å¼ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
    mp.set_start_method("spawn", force=True)
    main()